pipeline_config_id: runner_config

data:
  pendle:
    data_format: json
    train_dir:  ./data/pendle/train.json
    valid_dir:  ./data/pendle/dev.json
    test_dir:  ./data/pendle/test.json
    data_specs:
      num_event_types: 31
      pad_token_id: 31
      padding_side: right
      # padding_strategy: max_length
      # # truncation_strategy: longest_first # or Truncate to a maximum length specified with the argument `max_length`
      # max_len: 740



RMTPP_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: RMTPP # model name
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 20
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: -1
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 20
    dropout: 0.0
    use_ln: False
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

RMTPP_eval:
  base_config:
    stage: eval
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    base_dir: './checkpoints/'
    model_id: RMTPP
  trainer_config:
    batch_size: 256
    max_epoch: 1
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    sharing_param_layer: False
    use_ln: False
    seed: 2019
    gpu: 0
    pretrained_model_dir: ./checkpoints/2398033_135054637950784_251128-145329/models/saved_model
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 120 #7000 for pendle


RMTPP_gen:
  base_config:
    stage: eval
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    base_dir: './checkpoints/'
    model_id: RMTPP
  trainer_config:
    batch_size: 16  # Per GPU batch size
    max_epoch: 20
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-2
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
  model_config:
    hidden_size: 32
    time_embi_size: 16
    mc_num_sample_per_step: 20
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 20
    dropout: 0.0
    use_ln: False
    seed: 2019
    gpu: 0
    pretrained_model_dir: ./checkpoints/2398033_135054637950784_251128-145329/models/saved_model
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 120
      num_step_gen: 11

NHP_eval:
  base_config:
    stage: eval
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    base_dir: './checkpoints/'
    model_id: NHP
  trainer_config:
    batch_size: 64
    max_epoch: 1
  model_config:
    hidden_size: 64
    use_ln: False
    seed: 2019
    gpu: 0
    pretrained_model_dir: ./checkpoints/<your_checkpoint>/models/saved_model  # Replace with your trained model path
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 7000

NHP_gen:
  base_config:
    stage: eval
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: NHP # model name
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 64
    max_epoch: 1
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse']
    seed: 2019
    gpu: 0
  model_config:
    hidden_size: 64
    loss_integral_num_sample_per_step: 20
    pretrained_model_dir: ./checkpoints/<your_checkpoint>/models/saved_model  # Replace with your trained model path
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 120 # 7000
      num_step_gen: 5

FullyNN_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: FullyNN # model name
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 200
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
  model_config:
    rnn_type: LSTM
    hidden_size: 32
    time_emb_size: 4
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 20
    dropout: 0.0
    use_ln: False
    model_specs:
      num_mlp_layers: 3
#    thinning:
#      num_seq: 10
#      num_sample: 1
#      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
#      look_ahead_time: 10
#      patience_counter: 5 # the maximum iteration used in adaptive thinning
#      over_sample_rate: 5
#      num_samples_boundary: 5
#      dtime_max: 5
#      num_step_gen: 1

FullyNN_gen:
  base_config:
    stage: eval
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: FullyNN
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 1
    resume_checkpoint: True
    seed: 2019
    gpu: 0
  model_config:
    rnn_type: LSTM
    hidden_size: 32
    time_emb_size: 4
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 20
    pretrained_model_dir: ./checkpoints/<your_checkpoint>/models/saved_model  # Replace with your trained model path
    dropout: 0.0
    use_ln: False
    model_specs:
      num_mlp_layers: 3
      proper_marked_intensities: true
      use_hybrid_loss: false
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500
      look_ahead_time: 10
      patience_counter: 5
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 120
      num_step_gen: 11


IntensityFree_gen:
  base_config:
    stage: eval
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: IntensityFree # model name
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 200
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 20
    dropout: 0.0
    use_ln: False
    pretrained_model_dir: ./checkpoints/<your_checkpoint>/models/saved_model  # Replace with your trained model path
    model_specs:
      num_mix_components: 3
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 120 #7000
      num_step_gen: 5


ODETPP_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: ODETPP # model name
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 32
    max_epoch: 200
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-1
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: -1
  model_config:
    hidden_size: 4
    time_emb_size: 4
    num_layers: 1
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 20
    dropout: 0.0
    use_ln: False
    model_specs:
      ode_num_sample_per_step: 2
      time_factor: 100
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 50 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5
      num_step_gen: 1

ODETPP_gen:
  base_config:
    stage: eval
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    base_dir: './checkpoints/'
    model_id: ODETPP
  trainer_config:
    batch_size: 256
    max_epoch: 1
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 5
    dropout: 0.0
    use_ln: False
    seed: 2019
    gpu: 0
    pretrained_model_dir: ./checkpoints/2489410_131177477080896_251128-152246/models/saved_model
    model_specs:
      ode_num_sample_per_step: 2
      time_factor: 0.001
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 120 # 120
      num_step_gen: 5

NHP_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: NHP # model name
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 64
    max_epoch: 100
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-2
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: -1
  model_config:
    hidden_size: 64
    loss_integral_num_sample_per_step: 20
#    pretrained_model_dir: ./checkpoints/75518_4377527680_230530-132355/models/saved_model
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5
      num_step_gen: 1



SAHP_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: SAHP # model name
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 20
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    loss_integral_num_sample_per_step: 20
    use_ln: False
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5
      num_step_gen: 1



SAHP_gen:
  base_config:
    stage: eval
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: SAHP # model name
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 1
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    loss_integral_num_sample_per_step: 20
    pretrained_model_dir: ./checkpoints/<your_checkpoint>/models/saved_model  # Replace with your trained model path
    use_ln: False
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 120
      num_step_gen: 11

THP_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: THP # model name
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 30
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    loss_integral_num_sample_per_step: 20
    use_ln: False
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5
      num_step_gen: 1


THP_gen:
  base_config:
    stage: eval
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: THP # model name
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 1
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 20
    loss_integral_num_sample_per_step: 20
    use_ln: False
    pretrained_model_dir: ./checkpoints/2398923_126933671417664_251128-145432/models/saved_model
    thinning:
      num_seq: 10
      num_sample: 1
      num_exp: 500 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 120
      num_step_gen: 5 

AttNHP_train:
  base_config:
    stage: train
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: AttNHP # model name
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 200
    shuffle: False
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: -1
  model_config:
    hidden_size: 16
    time_emb_size: 4
    num_layers: 2
    num_heads: 2
    loss_integral_num_sample_per_step: 10
    use_ln: False
    thinning:
      num_seq: 2
      num_sample: 1
      num_exp: 50 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5
      num_step_gen: 1


AttNHP_gen:
  base_config:
    stage: eval
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: AttNHP # model name
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 1
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 1
    num_heads: 2
    mc_num_sample_per_step: 20
    loss_integral_num_sample_per_step: 20
    use_ln: False
    pretrained_model_dir: ./checkpoints/1355781_129148073428800_251208-084812/models/saved_model
    thinning:
      num_seq: 2
      num_sample: 1
      num_exp: 50 # number of i.i.d. Exp(intensity_bound) draws at one time in thinning algorithm
      look_ahead_time: 10
      patience_counter: 5 # the maximum iteration used in adaptive thinning
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 120
      num_step_gen: 11

