pipeline_config_id: runner_config

data:
  pendle:
    data_format: json
    train_dir:  ./data/pendle/train.json
    valid_dir:  ./data/pendle/dev.json
    test_dir:  ./data/pendle/test.json
    data_specs:
      num_event_types: 31
      pad_token_id: 31
      padding_side: right

NHP_train_multi_gpu:
  base_config:
    stage: train
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: NHP
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 128  # This will be adjusted by the script based on num GPUs
    max_epoch: 20
    shuffle: True
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0  # Will be overridden by multi-gpu script
    # Memory optimization settings
    gradient_accumulation_steps: 2
    gradient_checkpointing: False
    max_grad_norm: 5.0
  model_config:
    hidden_size: 64
    loss_integral_num_sample_per_step: 10  # Reduced for memory
    dropout: 0.1
    use_ln: True
    thinning:
      num_seq: 5  # Reduced for memory
      num_sample: 1
      num_exp: 100  # Reduced for memory
      look_ahead_time: 10
      patience_counter: 5
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5
      num_step_gen: 1

NHP_train_small:
  base_config:
    stage: train
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: NHP
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 32  # Smaller batch size for memory
    max_epoch: 20
    shuffle: True
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
  model_config:
    hidden_size: 32  # Smaller model
    loss_integral_num_sample_per_step: 5
    dropout: 0.1
    use_ln: True
    thinning:
      num_seq: 2
      num_sample: 1
      num_exp: 50
      look_ahead_time: 10
      patience_counter: 5
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5
      num_step_gen: 1

RMTPP_train_multi_gpu:
  base_config:
    stage: train
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: RMTPP
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 256
    max_epoch: 20
    shuffle: True
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 10
    sharing_param_layer: False
    loss_integral_num_sample_per_step: 10
    dropout: 0.1
    use_ln: True
    thinning:
      num_seq: 5
      num_sample: 1
      num_exp: 100
      look_ahead_time: 10
      patience_counter: 5
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5

THP_train_multi_gpu:
  base_config:
    stage: train
    backend: torch
    dataset_id: pendle
    runner_id: std_tpp
    model_id: THP
    base_dir: './checkpoints/'
  trainer_config:
    batch_size: 128
    max_epoch: 30
    shuffle: True
    optimizer: adam
    learning_rate: 1.e-3
    valid_freq: 1
    use_tfb: True
    metrics: [ 'acc', 'rmse' ]
    seed: 2019
    gpu: 0
  model_config:
    hidden_size: 32
    time_emb_size: 16
    num_layers: 2
    num_heads: 2
    mc_num_sample_per_step: 10
    loss_integral_num_sample_per_step: 10
    use_ln: True
    dropout: 0.1
    thinning:
      num_seq: 5
      num_sample: 1
      num_exp: 100
      look_ahead_time: 10
      patience_counter: 5
      over_sample_rate: 5
      num_samples_boundary: 5
      dtime_max: 5
      num_step_gen: 1